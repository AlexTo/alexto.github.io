<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-11T12:23:31+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex’s Tech Blog</title><subtitle>Document my learning journey</subtitle><author><name>Alex To</name></author><entry><title type="html">Locating Objects without bounding boxes (Ribera et al., 2019)</title><link href="http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes.html" rel="alternate" type="text/html" title="Locating Objects without bounding boxes (Ribera et al., 2019)" /><published>2020-11-09T08:39:00+11:00</published><updated>2020-11-09T08:39:00+11:00</updated><id>http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes</id><content type="html" xml:base="http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes.html">&lt;blockquote&gt;
  &lt;p&gt;In this paper , the authors proposed a loss function based on the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals or sliding windows.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#Ribera2019&quot;&gt;(Ribera et al., 2019)&lt;/a&gt; aim to achieve object localization without using bounding boxes. That means, instead of annotating the images with boxes, the ground truth labels are now points. The authors argued that using points as ground truth labels might be less laborious to obtain in some cases where bounding boxes are not required.&lt;/p&gt;

&lt;p&gt;Supposed we have ground truth labels as a set of points, let’s call it \(A\), we want to estimate a set \(B\) that is as close to \(A\) as possible. To train the network, we need to measure how far off the estimated set B is from A. We all know how to measure the distance between two points right?, but how to measure distance between two sets of points ? One way is to built on the idea of Hausdorff Distance. If you are not familiar with what Hausdorff Distance is, you may want to check out my previous blog post &lt;a href=&quot;/2020/11/09/hausdorff-distance.html&quot;&gt;here&lt;/a&gt; ;)&lt;/p&gt;

&lt;p&gt;Alright, so now we know that we can build a loss function based on Average Hausdorff Distance (AHD), let’s see why it might be tricky to construct a loss function from its original form. The AHD is defined by&lt;/p&gt;

\[d_{AH}(A, B) = \frac{1}{|A|}\sum_{a \in A}\min_{b \in B}d(a, b) + \frac{1}{|B|}\sum_{b \in B}\min_{a \in A}d(a, b)\]

&lt;p&gt;Note that in the paper, the authors used the notion of \(X\) and \(Y\) but I replaced them with \(A\) and \(B\) to avoid confusion with \((x, y)\) coordinates used later in this post.&lt;/p&gt;

&lt;p&gt;Now we have to think about the outputs of the network. Ideally, if we want to use \(d_{AH}\) as the loss function, we want the network to output a set of coordinates \((x, y)\), perhaps, at the final linear layer as illustrated in Figure 1.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/nobboxes_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 1&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The issue is that the size of the final linear layer is fixed, so the estimated number of points is fixed. This is not desirable because the number of points is different from image to image so let’s not do that. Another way is to let the network output a heatmap or probability map \(P\) where \(p_{x, y}\) is the probability that \((x, y)\) is a key point similar to &lt;a class=&quot;citation&quot; href=&quot;#Ronneberger2015&quot;&gt;(Ronneberger et al., 2015)&lt;/a&gt; as illustrated in Figure 2.&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/nobboxes_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 2&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The new output can predict any number of estimated points, but it no longer returns pixel coordinates. Hence, we need a bit of modification to the original AHD so that the loss function is differentiable with respect to the output \(P\).&lt;/p&gt;

&lt;p&gt;The authors proposed to replace AHD with an approximation&lt;/p&gt;

\[d_{WH}(p, Y) = \frac{1}{S + \epsilon}\sum_{a \in \Omega}p_a\min_{b \in B}d(a, b) + \frac{1}{|B|}\sum_{b \in B} \underset{a \in \Omega}{M_{\alpha}}[p_ad(a, b) + (1-p_a)d_{max}]\]

&lt;p&gt;where&lt;/p&gt;

\[S = \sum_{x \in \Omega}p_x\]

\[\underset{a \in \Omega}{M_{\alpha}}[f(a)] = \left(\frac{1}{|A|}\sum_{a \in A}f^{\alpha}(a)\right)^{\frac{1}{\alpha}}\]

&lt;p&gt;Quite a lot of things going on here so let’s break it down bit by bit.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Ribera2019&quot;&gt;Ribera, J., Guera, D., Chen, Y., &amp;amp; Delp, E. J. (2019). Locating objects without bounding boxes. &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, &lt;i&gt;2019-June&lt;/i&gt;. https://doi.org/10.1109/CVPR.2019.00664&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ronneberger2015&quot;&gt;Ronneberger, O., Fischer, P., &amp;amp; Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. &lt;i&gt;Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)&lt;/i&gt;, &lt;i&gt;9351&lt;/i&gt;, 234–241. https://doi.org/10.1007/978-3-319-24574-4_28&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="deep-learning" /><category term="computer-vision" /><category term="convolutional-neural-network" /><category term="hausdorff-distance" /><summary type="html">In this paper , the authors proposed a loss function based on the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals or sliding windows.</summary></entry><entry><title type="html">Hausdorff Distance</title><link href="http://localhost:4000/2020/11/09/hausdorff-distance.html" rel="alternate" type="text/html" title="Hausdorff Distance" /><published>2020-11-09T08:35:00+11:00</published><updated>2020-11-09T08:35:00+11:00</updated><id>http://localhost:4000/2020/11/09/hausdorff-distance</id><content type="html" xml:base="http://localhost:4000/2020/11/09/hausdorff-distance.html">&lt;blockquote&gt;
  &lt;p&gt;Hausdorff distance measures how far two set of points are from each other. Informally, it is the greatest of all the distances from a point in one set to the closest point in the other set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="maths" /><category term="hausdorff-distance" /><summary type="html">Hausdorff distance measures how far two set of points are from each other. Informally, it is the greatest of all the distances from a point in one set to the closest point in the other set.</summary></entry><entry><title type="html">Generative Adversarial Nets</title><link href="http://localhost:4000/2020/11/06/generative-adversarial-nets.html" rel="alternate" type="text/html" title="Generative Adversarial Nets" /><published>2020-11-06T00:44:00+11:00</published><updated>2020-11-06T00:44:00+11:00</updated><id>http://localhost:4000/2020/11/06/generative-adversarial-nets</id><content type="html" xml:base="http://localhost:4000/2020/11/06/generative-adversarial-nets.html"></content><author><name>Alex To</name></author><category term="gan" /><category term="computer-vision" /><category term="deep-learning" /><summary type="html"></summary></entry></feed>