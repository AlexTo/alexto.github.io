<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-28T00:49:34+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex’s Tech Blog</title><subtitle>Document my learning journey</subtitle><author><name>Alex To</name></author><entry><title type="html">Visual explanations from deep networks via gradient-based localization</title><link href="http://localhost:4000/2020/11/28/visual-explanations-from-deep-networks-via-gradient-based-localization.html" rel="alternate" type="text/html" title="Visual explanations from deep networks via gradient-based localization" /><published>2020-11-28T00:41:00+11:00</published><updated>2020-11-28T00:41:00+11:00</updated><id>http://localhost:4000/2020/11/28/visual-explanations-from-deep-networks-via-gradient-based-localization</id><content type="html" xml:base="http://localhost:4000/2020/11/28/visual-explanations-from-deep-networks-via-gradient-based-localization.html"></content><author><name>Alex To</name></author><category term="grad-cam" /><category term="deep-learning" /><category term="computer-vision" /><category term="convolutional-neural-network" /><summary type="html"></summary></entry><entry><title type="html">Learning deep features for discriminative localization</title><link href="http://localhost:4000/2020/11/27/learning-deep-features-for-discriminative-localization.html" rel="alternate" type="text/html" title="Learning deep features for discriminative localization" /><published>2020-11-27T22:20:00+11:00</published><updated>2020-11-27T22:20:00+11:00</updated><id>http://localhost:4000/2020/11/27/learning-deep-features-for-discriminative-localization</id><content type="html" xml:base="http://localhost:4000/2020/11/27/learning-deep-features-for-discriminative-localization.html">&lt;blockquote&gt;
  &lt;p&gt;In this paper, the authors discussed how Global Average Pooling enables localization ability despite being trained on image-level labels i.e without bounding boxes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;The main motivation of this paper &lt;a class=&quot;citation&quot; href=&quot;#Zhou2016&quot;&gt;(Zhou et al., 2016)&lt;/a&gt; is how to generate decision heatmap or class activation maps (CAM) in CNNs. Simply put, if the CNN model classifies an image with label Y, we would like to know which pixels from the original image trigger such decision (Figure 1).&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cam-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 1. Image taken from the paper&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One observation is that the convo layers actually behave as object detectors with deeper layers learn more abstract concepts. Despite not being train with bounding boxes, the convo layers actually have a remarkable ability to localize objects but this ability is lost when fully connected layers are used in the last few layers in traditional CNNs.  &lt;a href=&quot;/2020/11/21/global-average-pooling.html&quot;&gt;Global Average Pooling&lt;/a&gt; was introduced in &lt;a class=&quot;citation&quot; href=&quot;#Lin2014&quot;&gt;(Lin et al., 2014)&lt;/a&gt; to mitigate this problem. Removing FCs layers also reduces the number of parameters, thus, might reduce over-fitting.&lt;/p&gt;

&lt;p&gt;To use CAM, modification must be made to existing networks so that the &lt;mark&gt; final convo is followed by GAP and then a final FC layer. &lt;/mark&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For a given image, let \(f_k(x, y)\) represent the activation of unit k in the last convolutional layer at spatial location \((x, y)\). Then, for unit \(k\), the result of performing global average pooling, \(F^k\) is \(\sum_{x,y}f_k(x, y)\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cam-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 2&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As illustrated in Figure 2, \(f_k(x, y)\) is the single value at \((x, y)\) at the feature map \(k\). Since the convo layer is followed by GAP, \(F^k\) is the average of all \(f_k(x,y)\). We could also defined \(F^k\) as the sum of all \(f_k(x, y)\) as average is just the sum divided by a constant.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, after GAP, we add a final FC layer for classification. Let \(S_c\) be the activation for class \(c\) before softmax. Then \(S_c\) is just the dot product between the vector \(F\) and the weight vector for \(c\)&lt;/p&gt;

\[S_c = F \cdot w^c = \sum_{k}w^c_kF^k\]

&lt;p&gt;So far so good, we have established the “link” between \(S_c\) and vector \(F\). Let’s trace back one layer before \(F\) and establish the “link” between \(S_c\) and \((x, y)\). If we “extract” feature at location \((x, y)\) for every feature map to construct a vector \(f\) as shown in Figure 3, then notice that \(f\) has the same length as \(F\) and \(w^c\), obviously, due to GAP ;)&lt;/p&gt;

&lt;figure&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/cam-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We define, \(M_c(x, y)\), the importance of \((x, y)\) as the dot product between \(f\) and \(w^c\)&lt;/p&gt;

\[M_c(x,y) = \sum_kw^c_kf_k(x,y)\]

&lt;p&gt;Hence,&lt;/p&gt;

\[S_c = \sum_{x,y}M_c(x,y)\]

&lt;p&gt;Since, \(S_c\) is the sum of all \(M_c(x, y)\) then the larger \(M_c(x, y)\) is, the more it contributes to \(S_c\) that leads to the classification of an image to class \(c\).&lt;/p&gt;

&lt;p&gt;Finally, we upscale \(M_c\) to the original size of the image to get the class activation map ;)&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Zhou2016&quot;&gt;Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp;amp; Torralba, A. (2016). &lt;i&gt;Learning Deep Features for Discriminative Localization&lt;/i&gt;. &lt;i&gt;1&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Lin2014&quot;&gt;Lin, M., Chen, Q., &amp;amp; Yan, S. (2014). Network in network. &lt;i&gt;2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings&lt;/i&gt;, 1–10.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="cam" /><category term="deep-learning" /><category term="computer-vision" /><category term="convolutional-neural-network" /><summary type="html">In this paper, the authors discussed how Global Average Pooling enables localization ability despite being trained on image-level labels i.e without bounding boxes.</summary></entry><entry><title type="html">Global average pooling</title><link href="http://localhost:4000/2020/11/21/global-average-pooling.html" rel="alternate" type="text/html" title="Global average pooling" /><published>2020-11-21T14:25:00+11:00</published><updated>2020-11-21T14:25:00+11:00</updated><id>http://localhost:4000/2020/11/21/global-average-pooling</id><content type="html" xml:base="http://localhost:4000/2020/11/21/global-average-pooling.html">&lt;blockquote&gt;
  &lt;p&gt;Conventional CNNs perform convolution in the lower layers of the network. For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax layer. Global average pool is another strategy first proposed by (Lin et al., 2014)  to replace the traditional fully connected layers in CNNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;First, let’s have a look at a traditional CNN setup such as LeNet. The last two layers are the good old fully connected layers (FCs) before being connected to the final softmax layer.&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/lenet.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 1. CNN with fully connected layers&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Okay, but FCs are prone to over-fitting so we could either apply drop-out &lt;a class=&quot;citation&quot; href=&quot;#hinton2012improving&quot;&gt;(Hinton et al., 2012)&lt;/a&gt; or get rid of FCs altogether with average pooling after the last convolutional layer &lt;a class=&quot;citation&quot; href=&quot;#Lin2014&quot;&gt;(Lin et al., 2014)&lt;/a&gt; as illustrated in Figure 2.&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/lenet-gap.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 2. CNN with global average pooling&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;And that’s the whole idea about global average pooling, simple isn’t it ? ;)&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;hinton2012improving&quot;&gt;Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., &amp;amp; Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. &lt;i&gt;ArXiv Preprint ArXiv:1207.0580&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Lin2014&quot;&gt;Lin, M., Chen, Q., &amp;amp; Yan, S. (2014). Network in network. &lt;i&gt;2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings&lt;/i&gt;, 1–10.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="deep-learning" /><category term="computer-vision" /><category term="convolutional-neural-network" /><summary type="html">Conventional CNNs perform convolution in the lower layers of the network. For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax layer. Global average pool is another strategy first proposed by (Lin et al., 2014) to replace the traditional fully connected layers in CNNs.</summary></entry><entry><title type="html">Locating objects without bounding boxes</title><link href="http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes.html" rel="alternate" type="text/html" title="Locating objects without bounding boxes" /><published>2020-11-09T08:39:00+11:00</published><updated>2020-11-09T08:39:00+11:00</updated><id>http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes</id><content type="html" xml:base="http://localhost:4000/2020/11/09/locating-objects-without-bounding-boxes.html">&lt;blockquote&gt;
  &lt;p&gt;In this paper , the authors proposed a loss function based on the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals or sliding windows.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#Ribera2019&quot;&gt;(Ribera et al., 2019)&lt;/a&gt; aim to achieve object localization without using bounding boxes. That means, instead of annotating the images with boxes, the ground truth labels are now points. The authors argued that using points as ground truth labels might be less laborious to obtain in some cases where bounding boxes are not required.&lt;/p&gt;

&lt;p&gt;Supposed we have ground truth labels as a set of points, let’s call it \(A\), we want to estimate a set \(B\) that is as close to \(A\) as possible. To train the network, we need to measure how far off the estimated set B is from A. We all know how to measure the distance between two points right?, but how to measure distance between two sets of points ? One way is to built on the idea of &lt;a href=&quot;/2020/11/09/hausdorff-distance.html&quot;&gt;Hausdorff Distance&lt;/a&gt;.&lt;/p&gt;

\[d_{AH}(A, B) = \frac{1}{|A|}\sum_{a \in A}\min_{b \in B}d(a, b) + \frac{1}{|B|}\sum_{b \in B}\min_{a \in A}d(a, b)\]

&lt;p&gt;Note that in the paper, the authors used the notion of \(X\) and \(Y\) but I replaced them with \(A\) and \(B\) to avoid confusion with \((x, y)\) coordinates used later in this post.&lt;/p&gt;

&lt;p&gt;Now we have to think about the outputs of the network. Ideally, if we want to use \(d_{AH}\) as the loss function, we want the network to output a set of coordinates \((x, y)\), perhaps, at the final linear layer as illustrated in Figure 1.&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/nobboxes_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 1&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The issue is that the size of the final linear layer is fixed, so the estimated number of points is fixed. This is not desirable because the number of points is different from image to image so let’s not do that. Another way is to let the network output a heatmap or probability map \(P\) where \(p_{x, y}\) is the probability that \((x, y)\) is a key point similar to &lt;a class=&quot;citation&quot; href=&quot;#Ronneberger2015&quot;&gt;(Ronneberger et al., 2015)&lt;/a&gt; as illustrated in Figure 2.&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/nobboxes_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 2&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The new output can predict any number of estimated points, but it no longer returns pixel coordinates. Hence, we need a bit of modification to the original AHD so that the loss function is differentiable with respect to the output \(P\).&lt;/p&gt;

&lt;p&gt;The authors proposed to replace AHD with an approximation&lt;/p&gt;

\[d_{WH}(p, Y) = \frac{1}{S + \epsilon}\sum_{a \in \Omega}p_a\min_{b \in B}d(a, b) + \frac{1}{|B|}\sum_{b \in B} \underset{a \in \Omega}{M_{\alpha}}[p_ad(a, b) + (1-p_a)d_{max}]\]

&lt;p&gt;where&lt;/p&gt;

\[S = \sum_{x \in \Omega}p_x\]

\[\underset{a \in \Omega}{M_{\alpha}}[f(a)] = \left(\frac{1}{|A|}\sum_{a \in A}f^{\alpha}(a)\right)^{\frac{1}{\alpha}}\]

&lt;p&gt;Quite a lot of things going on here so let’s break it down bit by bit.&lt;/p&gt;

&lt;p&gt;Let&lt;/p&gt;

\[U = \frac{1}{S + \epsilon}\sum_{a \in \Omega}p_a\min_{b \in B}d(a, b)\]

\[V = \frac{1}{|B|}\sum_{b \in B} \underset{a \in \Omega}{M_{\alpha}}[p_ad(a, b) + (1-p_a)d_{max}]\]

&lt;p&gt;First of all, the intuition of this loss function is that if a predicted point \(a\) is far from set \(B\) then \(p_a\) must be low. But wait, why do we need both \(U\) and \(V\) ? Well, if \(V\) is absent then the model will predict an empty set \(A\) by setting all \(p_a\) to 0, that will minimize the cost to 0. Similarly if \(U\) is absent then the model will predict the entire image as set \(A\), that will also minimize the cost to 0 as the image contains set \(B\) so the distance between the image and set \(B\) is 0. Both terms must be there so that the model won’t predict too few or too many points than expected.&lt;/p&gt;

&lt;figure class=&quot;width_500&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/nobboxes_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;figcaption&gt;Figure 3&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;\(U\) is an approximation to 
\(\frac{1}{|A|}\sum_{a \in A}\min_{b \in B}d(a, b)\) so you might think we can do the same with \(V\) by letting&lt;/p&gt;

\[V = \frac{1}{B}\sum_{b \in B}\min_{b \in B}p_ad(a, b)\]

&lt;p&gt;However, unlike \(U\), now \(p_a\) appears inside the \(\min\) which makes the loss function not differentiable with respect to \(p\). A good approximation to the minimum function is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_mean&quot;&gt;generalized mean&lt;/a&gt;. You may find some good discussion about it &lt;a href=&quot;https://mathoverflow.net/questions/35191/a-differentiable-approximation-to-the-minimum-function&quot;&gt;here&lt;/a&gt; on Mathoverflow.&lt;/p&gt;

&lt;p&gt;One final thing to note is that the authors added a Smooth L1 Loss for the regression of the object count so the final loss becomes&lt;/p&gt;

\[\mathcal{L}(p, Y) =  d_{WH}(p, Y) + \mathcal{L}_{reg}(C - \hat{C})\]

&lt;p&gt;where \(C\) is the number of target points and \(\hat{C}\) is a scalar output from the network predicting the estimated object count.&lt;/p&gt;

&lt;p&gt;Hope you like my post and if there is any part that is not clear, leave a comment below, so I can update this post with more details ;)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Ribera2019&quot;&gt;Ribera, J., Guera, D., Chen, Y., &amp;amp; Delp, E. J. (2019). Locating objects without bounding boxes. &lt;i&gt;Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition&lt;/i&gt;, &lt;i&gt;2019-June&lt;/i&gt;. https://doi.org/10.1109/CVPR.2019.00664&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Ronneberger2015&quot;&gt;Ronneberger, O., Fischer, P., &amp;amp; Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. &lt;i&gt;Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)&lt;/i&gt;, &lt;i&gt;9351&lt;/i&gt;, 234–241. https://doi.org/10.1007/978-3-319-24574-4_28&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="deep-learning" /><category term="computer-vision" /><category term="convolutional-neural-network" /><category term="hausdorff-distance" /><summary type="html">In this paper , the authors proposed a loss function based on the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals or sliding windows.</summary></entry><entry><title type="html">Hausdorff Distance</title><link href="http://localhost:4000/2020/11/09/hausdorff-distance.html" rel="alternate" type="text/html" title="Hausdorff Distance" /><published>2020-11-09T08:35:00+11:00</published><updated>2020-11-09T08:35:00+11:00</updated><id>http://localhost:4000/2020/11/09/hausdorff-distance</id><content type="html" xml:base="http://localhost:4000/2020/11/09/hausdorff-distance.html">&lt;blockquote&gt;
  &lt;p&gt;Hausdorff distance measures how far two set of points are from each other. Informally, it is the greatest of all the distances from a point in one set to the closest point in the other set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;/ol&gt;</content><author><name>Alex To</name></author><category term="maths" /><category term="hausdorff-distance" /><summary type="html">Hausdorff distance measures how far two set of points are from each other. Informally, it is the greatest of all the distances from a point in one set to the closest point in the other set.</summary></entry></feed>